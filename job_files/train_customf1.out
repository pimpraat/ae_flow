wandb: Currently logged in as: pimpraat (dl2_ae_flow). Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/lcur1708/.netrc
wandb: wandb version 0.15.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.0
wandb: Run data is saved locally in /home/lcur1708/new_git_directory/ae_flow/wandb/run-20230525_180824-am6onrj8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wobbly-meadow-489
wandb: ⭐️ View project at https://wandb.ai/dl2_ae_flow/ae_flow
wandb: 🚀 View run at https://wandb.ai/dl2_ae_flow/ae_flow/runs/am6onrj8
/home/lcur1708/.conda/envs/dl2022/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/lcur1708/.conda/envs/dl2022/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Wide_ResNet50_2_Weights.IMAGENET1K_V1`. You can also use `weights=Wide_ResNet50_2_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
Running on subset: None
Number of abnormal vs normal samples in the threshold set: 3875 vs 1341
(5216,)
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [03:14<?, ?it/s]
Traceback (most recent call last):
  File "/home/lcur1708/new_git_directory/ae_flow/train.py", line 257, in <module>
    main(args)
  File "/home/lcur1708/new_git_directory/ae_flow/train.py", line 184, in main
    used_thr, best_model, current_best_score = model_checkpoint(epoch, model, experiment.threshold_loader_all, experiment.checkpoint_loader, current_best_score, used_thr, best_model, verbose=True)
  File "/home/lcur1708/new_git_directory/ae_flow/train.py", line 121, in model_checkpoint
    threshold = find_threshold(epoch, model, threshold_loader_all, verbose=False)
  File "/home/lcur1708/.conda/envs/dl2022/lib/python3.10/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/home/lcur1708/new_git_directory/ae_flow/train.py", line 97, in find_threshold
    optimal_threshold = optimize_threshold(anomaly_scores, true_labels)
  File "/home/lcur1708/new_git_directory/ae_flow/utils.py", line 36, in optimize_threshold
    print(true_labels.shape)
AttributeError: 'list' object has no attribute 'shape'
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: 
wandb: Run history:
wandb: Train loss per epoch: █▇▅▃▂▁
wandb:     flow loss (train) ████████▇▇▇▆▆▆▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▂▁▁▁▁
wandb:    recon_loss (train) █▇▇▆▅▄▄▄▃▃▃▃▃▃▃▃▃▃▂▂▂▄▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▅
wandb: 
wandb: Run summary:
wandb: Train loss per epoch: -2.26355
wandb:     flow loss (train) -4.99086
wandb:    recon_loss (train) 0.55896
wandb: 
wandb: 🚀 View run wobbly-meadow-489 at: https://wandb.ai/dl2_ae_flow/ae_flow/runs/am6onrj8
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230525_180824-am6onrj8/logs
srun: error: r29n3: task 0: Exited with exit code 1
